{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2109bd65-3cf5-4d93-9c83-075920e9b570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BLOB_STORAGE_ACCOUNT_NAME = \"pocspirelake\"\n",
    "BLOB_CONTAINER_URL = f\"{BLOB_STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "\n",
    "STORAGE_ACCOUNT_ACCESS_KEY_NAME = f\"fs.azure.account.key.{BLOB_CONTAINER_URL}\"\n",
    "\n",
    "# Read secret from Azure Key Vaultâ€“backed scope\n",
    "ACCOUNT_KEY = dbutils.secrets.get(scope = \"spireitopsStorageSecret\", key = \"dbstogare-acckey\")\n",
    "\n",
    "# Config to access storage account\n",
    "spark.conf.set(STORAGE_ACCOUNT_ACCESS_KEY_NAME, ACCOUNT_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec56a72-eb1a-4442-ab4c-34dadedac854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "isoRequestLogPath = f\"wasbs://silver@{BLOB_CONTAINER_URL}/bimrailsdb/iso_request_log.parquet\"\n",
    "isoRequestLogOutputDirPath = f\"wasbs://gold@{BLOB_CONTAINER_URL}/bimrailsdb\"\n",
    "\n",
    "# Read the Parquet file with the defined schema\n",
    "isoRequestLog_df = spark.read.parquet(isoRequestLogPath)\n",
    "#print(f\"Columns count: {len(isoRequestLog_df.columns)}\")\n",
    "#print(f\"Rows count: {isoRequestLog_df.count()}\")\n",
    "#df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f9428b75-f50c-476d-a25f-293d61e90da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.runtime import dbutils\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def write_single_parquet_file(df: DataFrame, fileName: str, output_path: str):\n",
    "    print(f\"Writing '{fileName}.parquet'\")\n",
    "    filepath = f\"{output_path}/{fileName}.parquet\"\n",
    "    temp_filepath = f\"{output_path}/temp_{fileName}.parquet\"\n",
    "\n",
    "    # Write a temporary folder containing one Parquet file\n",
    "    print(\"Write a temporary folder containing one Parquet file\")\n",
    "    df.repartition(1).write.format(\"parquet\").save(temp_filepath, mode=\"overwrite\")\n",
    "\n",
    "    # Find the Parquet file in the temporary folder\n",
    "    files = dbutils.fs.ls(temp_filepath)\n",
    "    output_file = next(x for x in files if x.name.startswith(\"part-\"))\n",
    "\n",
    "    print(\"Delete filepath, if it exists\")\n",
    "    # Delete filepath, if it exists\n",
    "    try:\n",
    "        dbutils.fs.ls(filepath)\n",
    "        # Filepath exists\n",
    "        print(f\"Deleting old {filepath}\")\n",
    "        dbutils.fs.rm(filepath, recurse=True)\n",
    "    except Exception as e:\n",
    "        if \"java.io.FileNotFoundException\" not in str(e):\n",
    "            raise e\n",
    "\n",
    "    # Move the Parquet file to the final location\n",
    "    dbutils.fs.mv(output_file.path, filepath)\n",
    "\n",
    "    # Delete temporary folder\n",
    "    print(\"Delete temporary folder\")\n",
    "    dbutils.fs.rm(temp_filepath, True)\n",
    "    print(f\"Wrote '{filepath}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e347b9ca-addd-4eb4-a16e-d0b1d534e6c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "write_single_parquet_file(isoRequestLog_df, \"iso_request_log\", isoRequestLogOutputDirPath)\n",
    "write_single_parquet_file(isoRequestLog_df.filter(col('ElapseTime') < 7000), \"iso_request_log_less_then_seven_second\", isoRequestLogOutputDirPath)\n",
    "write_single_parquet_file(isoRequestLog_df.filter((col('ElapseTime') >= 7000) & (col('ElapseTime') <= 10000)), \"iso_request_log_seven_to_ten_second\", isoRequestLogOutputDirPath)\n",
    "write_single_parquet_file(isoRequestLog_df.filter(col('ElapseTime') >= 10000), \"iso_request_log_more_than_ten_second\", isoRequestLogOutputDirPath)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SilverToGoldNotebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
